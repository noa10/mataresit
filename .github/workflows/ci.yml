name: Continuous Integration

on:
  push:
    branches: [ main, develop, feature/* ]
    paths:
      - 'src/**'
      - 'public/**'
      - 'package*.json'
      - 'vite.config.*'
      - 'tsconfig.json'
      - '.github/workflows/**'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'public/**'
      - 'package*.json'
      - 'vite.config.*'
      - 'tsconfig.json'
      - '.github/workflows/**'

env:
  NODE_VERSION: '18'
  CACHE_VERSION: 'v1'

jobs:
  # Setup and Dependency Caching
  setup:
    name: Setup Dependencies
    runs-on: ubuntu-latest
    outputs:
      cache-hit: ${{ steps.cache-deps.outputs.cache-hit }}
      node-cache-key: ${{ steps.cache-keys.outputs.node-key }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Generate cache keys
        id: cache-keys
        run: |
          echo "node-key=${{ env.CACHE_VERSION }}-${{ runner.os }}-node-${{ env.NODE_VERSION }}-${{ hashFiles('**/package-lock.json') }}" >> $GITHUB_OUTPUT

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Cache dependencies
        id: cache-deps
        uses: actions/cache@v3
        with:
          path: |
            ~/.npm
            node_modules
          key: ${{ steps.cache-keys.outputs.node-key }}
          restore-keys: |
            ${{ env.CACHE_VERSION }}-${{ runner.os }}-node-${{ env.NODE_VERSION }}-

      - name: Install dependencies
        if: steps.cache-deps.outputs.cache-hit != 'true'
        uses: nick-invision/retry@v2
        with:
          timeout_minutes: 10
          max_attempts: 3
          command: npm ci

  # Code Quality and Security Checks (Parallel)
  code-quality:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    needs: setup
    outputs:
      quality-passed: ${{ steps.quality-check.outputs.passed }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Restore dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.npm
            node_modules
          key: ${{ needs.setup.outputs.node-cache-key }}

      - name: Install dependencies (if cache miss)
        if: needs.setup.outputs.cache-hit != 'true'
        run: npm ci

      - name: Run ESLint
        id: eslint
        run: |
          npm run lint 2>&1 | tee eslint-results.txt
          if [[ ${PIPESTATUS[0]} -eq 0 ]]; then
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "passed=false" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Run TypeScript check
        id: typescript
        run: |
          npx tsc --noEmit 2>&1 | tee typescript-results.txt
          if [[ ${PIPESTATUS[0]} -eq 0 ]]; then
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "passed=false" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Security audit
        id: security-audit
        run: |
          npm audit --audit-level=moderate 2>&1 | tee security-audit.txt
          if [[ ${PIPESTATUS[0]} -eq 0 ]]; then
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "passed=false" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Check for secrets
        id: secrets-check
        uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: main
          head: HEAD
          extra_args: --debug --only-verified
        continue-on-error: true

      - name: Validate security scanning configuration
        run: |
          echo "ðŸ”’ Validating security scanning configuration..."

          # Check if required secrets are configured for security scanning
          MISSING_SECRETS=()

          # Check Snyk token
          if [[ -z "${{ secrets.SNYK_TOKEN }}" ]]; then
            MISSING_SECRETS+=("SNYK_TOKEN")
          fi

          # Check Supabase secrets for security validation
          if [[ -z "${{ secrets.SUPABASE_URL }}" ]]; then
            MISSING_SECRETS+=("SUPABASE_URL")
          fi

          if [[ ${#MISSING_SECRETS[@]} -gt 0 ]]; then
            echo "::warning::Missing security scanning secrets:"
            for secret in "${MISSING_SECRETS[@]}"; do
              echo "::warning::  - $secret"
            done
            echo "::warning::See .github/docs/SECURITY_SCANNING_SETUP.md for setup instructions"
          else
            echo "âœ… Security scanning configuration is complete"
          fi

      - name: Evaluate quality checks
        id: quality-check
        run: |
          QUALITY_PASSED=true

          if [[ "${{ steps.eslint.outputs.passed }}" != "true" ]]; then
            echo "ESLint check failed"
            QUALITY_PASSED=false
          fi

          if [[ "${{ steps.typescript.outputs.passed }}" != "true" ]]; then
            echo "TypeScript check failed"
            QUALITY_PASSED=false
          fi

          if [[ "${{ steps.security-audit.outputs.passed }}" != "true" ]]; then
            echo "Security audit failed"
            QUALITY_PASSED=false
          fi

          if [[ "${{ steps.secrets-check.outcome }}" == "failure" ]]; then
            echo "Secrets check failed"
            QUALITY_PASSED=false
          fi

          echo "passed=$QUALITY_PASSED" >> $GITHUB_OUTPUT

      - name: Upload quality reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: quality-reports-${{ github.run_number }}
          path: |
            eslint-results.txt
            typescript-results.txt
            security-audit.txt
          retention-days: 7

  # Testing Suite (Parallel)
  test:
    name: Test Suite
    runs-on: ubuntu-latest
    needs: setup
    environment: Production
    outputs:
      tests-passed: ${{ steps.test-results.outputs.passed }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Restore dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.npm
            node_modules
          key: ${{ needs.setup.outputs.node-cache-key }}

      - name: Install dependencies (if cache miss)
        if: needs.setup.outputs.cache-hit != 'true'
        run: npm ci

      - name: Setup test environment
        run: |
          # Create basic test setup if script exists
          if [ -f "tests/phase4-integration/scripts/setup-test-environment.js" ]; then
            npm run test:phase4:setup
          else
            echo "Test setup script not found, creating basic test environment"
            mkdir -p test-results
          fi
        env:
          TEST_SUPABASE_URL: ${{ secrets.TEST_SUPABASE_URL }}
          TEST_SUPABASE_ANON_KEY: ${{ secrets.TEST_SUPABASE_ANON_KEY }}
          TEST_SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.TEST_SUPABASE_SERVICE_ROLE_KEY }}
          TEST_GEMINI_API_KEY: ${{ secrets.TEST_GEMINI_API_KEY }}

      - name: Run unit tests
        id: unit-tests
        run: |
          # Run unit tests if they exist
          if npm run test:unit --dry-run 2>/dev/null; then
            npm run test:unit 2>&1 | tee unit-test-results.txt
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "Unit tests not configured, skipping"
            echo "passed=true" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Run integration tests
        id: integration-tests
        run: |
          # Run integration tests if they exist
          if npm run test:integration --dry-run 2>/dev/null; then
            npm run test:integration 2>&1 | tee integration-test-results.txt
            if [[ ${PIPESTATUS[0]} -eq 0 ]]; then
              echo "passed=true" >> $GITHUB_OUTPUT
            else
              echo "passed=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "Integration tests not configured, skipping"
            echo "passed=true" >> $GITHUB_OUTPUT
          fi
        env:
          TEST_SUPABASE_URL: ${{ secrets.TEST_SUPABASE_URL }}
          TEST_SUPABASE_ANON_KEY: ${{ secrets.TEST_SUPABASE_ANON_KEY }}
          TEST_SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.TEST_SUPABASE_SERVICE_ROLE_KEY }}
          TEST_GEMINI_API_KEY: ${{ secrets.TEST_GEMINI_API_KEY }}
        continue-on-error: true

      - name: Test Supabase connectivity
        run: |
          echo "Testing Supabase connectivity..."
          if [ -n "${{ secrets.TEST_SUPABASE_URL }}" ]; then
            curl -f "${{ secrets.TEST_SUPABASE_URL }}/health" || echo "Supabase health check failed"
          else
            echo "TEST_SUPABASE_URL not configured"
          fi

      - name: Evaluate test results
        id: test-results
        run: |
          TESTS_PASSED=true

          if [[ "${{ steps.unit-tests.outputs.passed }}" != "true" ]]; then
            echo "Unit tests failed"
            TESTS_PASSED=false
          fi

          if [[ "${{ steps.integration-tests.outputs.passed }}" != "true" ]]; then
            echo "Integration tests failed"
            TESTS_PASSED=false
          fi

          echo "passed=$TESTS_PASSED" >> $GITHUB_OUTPUT

      - name: Generate test reports
        if: always()
        run: |
          # Generate reports if script exists
          if [ -f "tests/phase4-integration/scripts/generate-reports.js" ]; then
            npm run test:phase4:report
          else
            echo "Test report generator not found, creating basic report"
            mkdir -p test-results
            echo "Test run completed at $(date)" > test-results/summary.txt
          fi

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ github.run_number }}
          path: |
            test-results/
            unit-test-results.txt
            integration-test-results.txt
          retention-days: 30

  # Build and Validate (Parallel)
  build:
    name: Build Application
    runs-on: ubuntu-latest
    needs: setup
    outputs:
      build-success: ${{ steps.build-check.outputs.success }}
      build-cache-key: ${{ steps.build-cache.outputs.cache-key }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Restore dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.npm
            node_modules
          key: ${{ needs.setup.outputs.node-cache-key }}

      - name: Install dependencies (if cache miss)
        if: needs.setup.outputs.cache-hit != 'true'
        run: npm ci

      - name: Generate build cache key
        id: build-cache
        run: |
          echo "cache-key=${{ env.CACHE_VERSION }}-${{ runner.os }}-build-${{ github.sha }}" >> $GITHUB_OUTPUT

      - name: Cache build output
        uses: actions/cache@v3
        with:
          path: dist/
          key: ${{ steps.build-cache.outputs.cache-key }}

      - name: Build application
        id: build-app
        run: |
          npm run build 2>&1 | tee build-output.txt
          if [[ ${PIPESTATUS[0]} -eq 0 ]]; then
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "success=false" >> $GITHUB_OUTPUT
          fi

      - name: Validate build output
        run: |
          if [ -d "dist" ]; then
            echo "âœ… Build output directory exists"
            ls -la dist/
          else
            echo "âŒ Build output directory not found"
            exit 1
          fi

      - name: Check build size
        run: |
          if [ -d "dist" ]; then
            BUILD_SIZE=$(du -sh dist/ | cut -f1)
            echo "ðŸ“¦ Build size: $BUILD_SIZE"

            # Check if build is reasonable size (under 50MB)
            SIZE_BYTES=$(du -sb dist/ | cut -f1)
            if [ $SIZE_BYTES -gt 52428800 ]; then
              echo "âš ï¸ Warning: Build size is large (>50MB)"
            fi
          fi

      - name: Evaluate build
        id: build-check
        run: |
          if [[ "${{ steps.build-app.outputs.success }}" == "true" ]]; then
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "success=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        if: steps.build-app.outputs.success == 'true'
        with:
          name: build-artifacts-${{ github.run_number }}
          path: |
            dist/
            package.json
            package-lock.json
            build-output.txt
          retention-days: 7

  # Vercel Preview Deployment (for PRs)
  vercel-preview:
    name: Vercel Preview Deployment
    runs-on: ubuntu-latest
    needs: [setup, build]
    if: github.event_name == 'pull_request' && needs.build.outputs.build-success == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Restore dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.npm
            node_modules
          key: ${{ needs.setup.outputs.node-cache-key }}

      - name: Restore build output
        uses: actions/cache@v3
        with:
          path: dist/
          key: ${{ needs.build.outputs.build-cache-key }}

      - name: Install dependencies (if cache miss)
        if: needs.setup.outputs.cache-hit != 'true'
        run: npm ci

      - name: Build (if cache miss)
        if: steps.restore-build.outputs.cache-hit != 'true'
        run: npm run build

      - name: Check Vercel secrets
        id: vercel-check
        run: |
          if [[ -n "${{ secrets.VERCEL_TOKEN }}" && -n "${{ secrets.VERCEL_ORG_ID }}" && -n "${{ secrets.VERCEL_PROJECT_ID }}" ]]; then
            echo "vercel-configured=true" >> $GITHUB_OUTPUT
            echo "âœ… Vercel secrets are configured"
          else
            echo "vercel-configured=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ Vercel secrets not configured - skipping preview deployment"
            echo ""
            echo "To enable Vercel preview deployments, configure these secrets:"
            echo "- VERCEL_TOKEN"
            echo "- VERCEL_ORG_ID"
            echo "- VERCEL_PROJECT_ID"
          fi

      - name: Deploy to Vercel Preview
        if: steps.vercel-check.outputs.vercel-configured == 'true'
        uses: amondnet/vercel-action@v25
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
          working-directory: ./
          scope: ${{ secrets.VERCEL_ORG_ID }}

      - name: Log preview deployment skipped
        if: steps.vercel-check.outputs.vercel-configured != 'true'
        run: |
          echo "â„¹ï¸ Vercel preview deployment skipped due to missing secrets"
          echo "The build completed successfully, but preview deployment requires Vercel configuration."

  # Deployment Readiness Gate
  deployment-gate:
    name: Deployment Readiness Gate
    runs-on: ubuntu-latest
    needs: [code-quality, test, build]
    outputs:
      deployment-ready: ${{ steps.gate-check.outputs.ready }}
      gate-status: ${{ steps.gate-check.outputs.status }}
      performance-score: ${{ steps.performance-check.outputs.score }}

    steps:
      - name: Evaluate deployment readiness
        id: gate-check
        run: |
          DEPLOYMENT_READY=true
          GATE_STATUS="âœ… All checks passed"

          # Check code quality
          if [[ "${{ needs.code-quality.outputs.quality-passed }}" != "true" ]]; then
            if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
              echo "âŒ Code quality checks failed (blocking for main branch)"
              DEPLOYMENT_READY=false
              GATE_STATUS="âŒ Code quality checks failed"
            else
              echo "âš ï¸ Code quality checks failed (non-blocking for feature branches)"
            fi
          fi

          # Check tests
          if [[ "${{ needs.test.outputs.tests-passed }}" != "true" ]]; then
            echo "âŒ Tests failed"
            DEPLOYMENT_READY=false
            GATE_STATUS="âŒ Tests failed"
          fi

          # Check build
          if [[ "${{ needs.build.outputs.build-success }}" != "true" ]]; then
            echo "âŒ Build failed"
            DEPLOYMENT_READY=false
            GATE_STATUS="âŒ Build failed"
          fi

          echo "ready=$DEPLOYMENT_READY" >> $GITHUB_OUTPUT
          echo "status=$GATE_STATUS" >> $GITHUB_OUTPUT

      - name: Performance check
        id: performance-check
        run: |
          # Calculate performance score based on workflow execution
          WORKFLOW_START="${{ github.event.head_commit.timestamp }}"
          CURRENT_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

          # Simple performance scoring (placeholder)
          PERFORMANCE_SCORE=85
          echo "score=$PERFORMANCE_SCORE" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Performance score: $PERFORMANCE_SCORE/100"

      - name: Update deployment status
        run: |
          echo "## ðŸš¦ Deployment Readiness Gate" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Status**: ${{ steps.gate-check.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Check Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Code Quality**: ${{ needs.code-quality.outputs.quality-passed == 'true' && 'âœ… Passed' || 'âš ï¸ Failed' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Tests**: ${{ needs.test.outputs.tests-passed == 'true' && 'âœ… Passed' || 'âŒ Failed' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Build**: ${{ needs.build.outputs.build-success == 'true' && 'âœ… Passed' || 'âŒ Failed' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ "${{ steps.gate-check.outputs.ready }}" == "true" ]]; then
            echo "ðŸŽ‰ **Ready for deployment!**" >> $GITHUB_STEP_SUMMARY
            if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
              echo "ðŸš€ **Production deployment will be triggered automatically**" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "ðŸš« **Not ready for deployment. Please fix the failing checks.**" >> $GITHUB_STEP_SUMMARY
          fi

  # Trigger production deployment (for main branch)
  trigger-deployment:
    name: Trigger Production Deployment
    runs-on: ubuntu-latest
    needs: [deployment-gate]
    if: github.ref == 'refs/heads/main' && needs.deployment-gate.outputs.deployment-ready == 'true'

    steps:
      - name: Trigger Vercel deployment
        uses: actions/github-script@v7
        with:
          script: |
            const result = await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'vercel-deploy.yml',
              ref: 'main',
              inputs: {
                environment: 'production'
              }
            });

            console.log('âœ… Vercel deployment workflow triggered');
            console.log(`Workflow dispatch ID: ${result.data.id || 'N/A'}`);
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
