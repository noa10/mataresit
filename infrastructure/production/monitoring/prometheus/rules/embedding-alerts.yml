# Prometheus Alerting Rules for Embedding Processing
# Comprehensive alerts for production monitoring

groups:
- name: embedding.rules
  interval: 30s
  rules:
  
  # Recording rules for embedding metrics
  - record: embedding:success_rate
    expr: rate(embedding_jobs_processed_total{status="success"}[5m]) / rate(embedding_jobs_processed_total[5m])
    labels:
      service: embedding
  
  - record: embedding:processing_rate
    expr: rate(embedding_jobs_processed_total[5m]) * 60
    labels:
      service: embedding
  
  - record: embedding:error_rate
    expr: rate(embedding_jobs_processed_total{status="failed"}[5m]) / rate(embedding_jobs_processed_total[5m])
    labels:
      service: embedding
  
  - record: embedding:avg_processing_time
    expr: rate(embedding_processing_duration_seconds_sum[5m]) / rate(embedding_processing_duration_seconds_count[5m])
    labels:
      service: embedding
  
  - record: embedding:p95_processing_time
    expr: histogram_quantile(0.95, rate(embedding_processing_duration_seconds_bucket[5m]))
    labels:
      service: embedding

- name: embedding.alerts
  rules:
  
  # Critical alerts
  - alert: EmbeddingServiceDown
    expr: up{job="paperless-maverick"} == 0
    for: 1m
    labels:
      severity: critical
      service: embedding
      team: platform
    annotations:
      summary: "Embedding service is down"
      description: "The embedding service has been down for more than 1 minute"
      runbook_url: "https://docs.company.com/runbooks/embedding-service-down"
      dashboard_url: "https://grafana.company.com/d/embedding-overview"
  
  - alert: EmbeddingSuccessRateLow
    expr: embedding:success_rate < 0.95
    for: 5m
    labels:
      severity: critical
      service: embedding
      team: platform
    annotations:
      summary: "Embedding success rate is below 95%"
      description: "Embedding success rate is {{ $value | humanizePercentage }} for more than 5 minutes"
      runbook_url: "https://docs.company.com/runbooks/embedding-success-rate-low"
  
  - alert: EmbeddingQueueDepthCritical
    expr: embedding_queue_depth > 100
    for: 2m
    labels:
      severity: critical
      service: embedding
      team: platform
    annotations:
      summary: "Embedding queue depth is critically high"
      description: "Queue depth is {{ $value }} items, above critical threshold of 100"
      runbook_url: "https://docs.company.com/runbooks/queue-depth-high"
  
  - alert: EmbeddingProcessingStalled
    expr: rate(embedding_jobs_processed_total[5m]) == 0 and embedding_queue_depth > 0
    for: 3m
    labels:
      severity: critical
      service: embedding
      team: platform
    annotations:
      summary: "Embedding processing has stalled"
      description: "No jobs processed in 5 minutes but queue has {{ $value }} items"
      runbook_url: "https://docs.company.com/runbooks/processing-stalled"
  
  # Warning alerts
  - alert: EmbeddingSuccessRateWarning
    expr: embedding:success_rate < 0.98 and embedding:success_rate >= 0.95
    for: 10m
    labels:
      severity: warning
      service: embedding
      team: platform
    annotations:
      summary: "Embedding success rate is below 98%"
      description: "Embedding success rate is {{ $value | humanizePercentage }} for more than 10 minutes"
  
  - alert: EmbeddingQueueDepthHigh
    expr: embedding_queue_depth > 50 and embedding_queue_depth <= 100
    for: 5m
    labels:
      severity: warning
      service: embedding
      team: platform
    annotations:
      summary: "Embedding queue depth is high"
      description: "Queue depth is {{ $value }} items, above warning threshold of 50"
  
  - alert: EmbeddingProcessingTimeSlow
    expr: embedding:p95_processing_time > 300
    for: 10m
    labels:
      severity: warning
      service: embedding
      team: platform
    annotations:
      summary: "Embedding processing time is slow"
      description: "95th percentile processing time is {{ $value }}s for more than 10 minutes"
  
  - alert: EmbeddingErrorRateHigh
    expr: embedding:error_rate > 0.05
    for: 5m
    labels:
      severity: warning
      service: embedding
      team: platform
    annotations:
      summary: "Embedding error rate is high"
      description: "Error rate is {{ $value | humanizePercentage }} for more than 5 minutes"
  
  # API and rate limiting alerts
  - alert: APIQuotaUsageHigh
    expr: api_quota_usage_percent{provider="gemini", type="requests"} > 80
    for: 5m
    labels:
      severity: warning
      service: embedding
      team: platform
    annotations:
      summary: "API quota usage is high"
      description: "{{ $labels.provider }} {{ $labels.type }} quota usage is {{ $value }}%"
  
  - alert: APIQuotaUsageCritical
    expr: api_quota_usage_percent{provider="gemini", type="requests"} > 95
    for: 1m
    labels:
      severity: critical
      service: embedding
      team: platform
    annotations:
      summary: "API quota usage is critical"
      description: "{{ $labels.provider }} {{ $labels.type }} quota usage is {{ $value }}%"
  
  - alert: RateLimitExceeded
    expr: rate(rate_limit_exceeded_total[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
      service: embedding
      team: platform
    annotations:
      summary: "Rate limits are being exceeded"
      description: "Rate limit exceeded {{ $value }} times per second for {{ $labels.provider }}"
  
  # Cost monitoring alerts
  - alert: EmbeddingCostHigh
    expr: rate(api_cost_total{provider="gemini"}[1h]) * 24 > 100
    for: 30m
    labels:
      severity: warning
      service: embedding
      team: platform
    annotations:
      summary: "Daily embedding cost is high"
      description: "Projected daily cost is ${{ $value }} for {{ $labels.provider }}"
  
  - alert: EmbeddingCostCritical
    expr: rate(api_cost_total{provider="gemini"}[1h]) * 24 > 500
    for: 15m
    labels:
      severity: critical
      service: embedding
      team: platform
    annotations:
      summary: "Daily embedding cost is critical"
      description: "Projected daily cost is ${{ $value }} for {{ $labels.provider }}"

- name: worker.alerts
  rules:
  
  # Worker health alerts
  - alert: WorkerDown
    expr: up{job="embedding-queue-workers"} == 0
    for: 1m
    labels:
      severity: critical
      service: worker
      team: platform
    annotations:
      summary: "Embedding worker is down"
      description: "Worker {{ $labels.instance }} has been down for more than 1 minute"
      runbook_url: "https://docs.company.com/runbooks/worker-down"
  
  - alert: WorkerHighCPU
    expr: rate(container_cpu_usage_seconds_total{pod=~"embedding-queue-workers-.*"}[5m]) * 100 > 80
    for: 10m
    labels:
      severity: warning
      service: worker
      team: platform
    annotations:
      summary: "Worker CPU usage is high"
      description: "Worker {{ $labels.pod }} CPU usage is {{ $value }}% for more than 10 minutes"
  
  - alert: WorkerHighMemory
    expr: container_memory_usage_bytes{pod=~"embedding-queue-workers-.*"} / container_spec_memory_limit_bytes * 100 > 85
    for: 10m
    labels:
      severity: warning
      service: worker
      team: platform
    annotations:
      summary: "Worker memory usage is high"
      description: "Worker {{ $labels.pod }} memory usage is {{ $value }}% for more than 10 minutes"
  
  - alert: WorkerRestartLoop
    expr: rate(kube_pod_container_status_restarts_total{pod=~"embedding-queue-workers-.*"}[15m]) > 0
    for: 5m
    labels:
      severity: warning
      service: worker
      team: platform
    annotations:
      summary: "Worker is restarting frequently"
      description: "Worker {{ $labels.pod }} has restarted {{ $value }} times in the last 15 minutes"
  
  - alert: WorkerHealthCheckFailing
    expr: probe_success{job="blackbox", instance=~".*embedding-queue-workers.*"} == 0
    for: 2m
    labels:
      severity: critical
      service: worker
      team: platform
    annotations:
      summary: "Worker health check is failing"
      description: "Health check for {{ $labels.instance }} has been failing for more than 2 minutes"
  
  # Scaling alerts
  - alert: WorkerScalingNeeded
    expr: embedding_queue_depth / count(up{job="embedding-queue-workers"} == 1) > 20
    for: 5m
    labels:
      severity: warning
      service: worker
      team: platform
    annotations:
      summary: "Worker scaling may be needed"
      description: "Queue depth per worker is {{ $value }}, consider scaling up"
  
  - alert: WorkerUnderUtilized
    expr: avg(rate(worker_jobs_processed_total[5m])) * 60 < 1 and count(up{job="embedding-queue-workers"} == 1) > 2
    for: 30m
    labels:
      severity: info
      service: worker
      team: platform
    annotations:
      summary: "Workers may be under-utilized"
      description: "Average job processing rate is {{ $value }} jobs/minute with {{ $labels.count }} workers"

- name: system.alerts
  rules:
  
  # Database alerts
  - alert: DatabaseConnectionsHigh
    expr: database_connections_active / database_connections_max > 0.8
    for: 5m
    labels:
      severity: warning
      service: database
      team: platform
    annotations:
      summary: "Database connections are high"
      description: "Database connection usage is {{ $value | humanizePercentage }}"
  
  - alert: DatabaseQueryTimeSlow
    expr: histogram_quantile(0.95, rate(database_query_duration_seconds_bucket[5m])) > 1
    for: 10m
    labels:
      severity: warning
      service: database
      team: platform
    annotations:
      summary: "Database queries are slow"
      description: "95th percentile query time is {{ $value }}s for more than 10 minutes"
  
  # Disk space alerts
  - alert: DiskSpaceHigh
    expr: (node_filesystem_size_bytes - node_filesystem_avail_bytes) / node_filesystem_size_bytes > 0.8
    for: 5m
    labels:
      severity: warning
      service: system
      team: platform
    annotations:
      summary: "Disk space usage is high"
      description: "Disk usage on {{ $labels.instance }} is {{ $value | humanizePercentage }}"
  
  - alert: DiskSpaceCritical
    expr: (node_filesystem_size_bytes - node_filesystem_avail_bytes) / node_filesystem_size_bytes > 0.9
    for: 2m
    labels:
      severity: critical
      service: system
      team: platform
    annotations:
      summary: "Disk space usage is critical"
      description: "Disk usage on {{ $labels.instance }} is {{ $value | humanizePercentage }}"
